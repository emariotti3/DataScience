{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1) Una red social almacena el contenido de los chats entre sus usuarios en un RDD que tiene registros con el siguiente \n",
    "#formato: (chat_id, user_id, nickname, text). Queremos saber cuál es el usuario (user_id) que mas preguntas hace en sus \n",
    "#chats, contabilizamos una pregunta por cada caracter “?” que aparezca en el campo text. Programar en Spark un programa \n",
    "#que identifique al usuario preguntón. (*) (15 pts)\n",
    "\n",
    "chatsData = [(1, 1, \"euge93\",\"what is data science?\"), \n",
    "             (2, 2, \"abrden\", \"its the science that studies data\"),\n",
    "             (3, 1, \"euge93\",\"really?\"),\n",
    "             (4, 3, \"czarnia\",\"are you sure?\"),\n",
    "             (5, 1, \"euge93\",\"sounds like she is right\"),\n",
    "             (6, 3, \"czarnia\",\"oh, allright then\"),\n",
    "             (7, 3, \"czarnia\",\"is the exam next week?\"),\n",
    "             (8, 1, \"euge93\",\"on Thursday\"),\n",
    "             (9, 3, \"czarnia\",\"should we practice PySpark???\"),\n",
    "             (10, 2, \"abrden\", \"yeah\"),\n",
    "            ]\n",
    "chatsRDD = sc.parallelize(chatsData, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatsRDD.filter(lambda row: \"?\" in row[3]).map(lambda row: (row[1], row[3].count(\"?\")))\\\n",
    ".reduceByKey(lambda row1, row2: row1+row2)\\\n",
    ".reduce(lambda row1, row2: row1 if (row1[1] > row2[1]) else row2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UBER almacena en un cluster todos los datos sobre el movimiento y viajes de todos sus vehículos. \n",
    "#Existe un proceso que nos devuelve un RDD llamado trip_summary con los siguientes campos: \n",
    "#(driver_id, car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled), Programar usando Py Spark \n",
    "#un programa que nos indique cual fue el conductor con mayor promedio de distancia recorrida por viaje para \n",
    "#Abril de 2016. (***) (15 pts)\n",
    "import datetime\n",
    "\n",
    "tripsData = [(101202, 100, 234, \"pass12\", datetime.date(2015, 10, 17),25), \n",
    "             (30000, 202, 501, \"pass13\", datetime.date(2016,4, 18),50),\n",
    "             (101202, 100, 4000, \"pass14\", datetime.date(2016, 4, 24),4), \n",
    "             (30000, 202, 3500, \"pass15\", datetime.date(2016, 4, 12),3),\n",
    "             (101202, 100, 4002, \"pass16\", datetime.date(2016, 4, 21),10), \n",
    "             (12000, 5000, 9000, \"pass17\", datetime.date(2016, 4, 22),9),\n",
    "             (12000, 5000, 90300, \"pass18\", datetime.date(2016, 4, 23),11),\n",
    "             (30000, 202, 7000, \"pass19\", datetime.date(2016, 4, 24),8),\n",
    "             (101202, 100, 9033, \"pass20\", datetime.date(2016, 4, 24),6), \n",
    "             (30000, 202, 8000, \"pass21\", datetime.date(2016, 4, 24),20)\n",
    "            ]\n",
    "trip_summary = sc.parallelize(tripsData, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.25, 30000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_dist(row):\n",
    "    if (row[1][1] == 0): return 0\n",
    "    return row[1][0] / float(row[1][1])\n",
    "\n",
    "trip_summary.filter(lambda row: row[4] <= datetime.date(2016,4,30) and row[4] >= datetime.date(2016, 4, 1))\\\n",
    "            .map(lambda row:(row[0], (row[5], 1)))\\\n",
    "            .reduceByKey(lambda row1, row2: (row1[0] + row2[0], row1[1] + row2[1]))\\\n",
    "            .map(lambda row: (avg_dist(row) , row[0])).reduce(lambda row1, row2: row1 if (row1[0] > row2[0]) else row2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODOS:\n",
      "[('abcd', 3), ('a', 2), ('abacde', 4), ('abcdefghi', 1), ('abcde', 2), ('abcdefbg', 1), ('abcdefb', 1), ('abac', 1), ('ab', 5), ('abcbd', 1), ('abcdefcghib', 1), ('abc', 3), ('abcdefg', 1), ('abcdef', 1)]\n",
      "MAS FRECUENTE:\n",
      "('ab', 5)\n",
      "TOP FIVE:[('ab', 5), ('abacde', 4), ('abcd', 3), ('abc', 3), ('a', 2)]\n",
      "ORDENADOS ALFABÉTICAMENE:[('a', 2), ('ab', 5), ('abac', 1), ('abacde', 4), ('abc', 3), ('abcbd', 1), ('abcd', 3), ('abcde', 2), ('abcdef', 1), ('abcdefb', 1), ('abcdefbg', 1), ('abcdefcghib', 1), ('abcdefg', 1), ('abcdefghi', 1)]\n",
      "INVERSE RANK:[('abcdefghi', 1), ('abcdefbg', 1), ('abcdefb', 1), ('abac', 1), ('abcbd', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Tenemos una colección de documentos (textos) almacenados en un cluster. Se quiere establecer un ranking de los patrones\n",
    "#mas frecuentes para la aparición de letras en las palabras. Por ejemplo “sister”, “ejects” , “ninety” y “amazon” \n",
    "#responden al patrón “abacde”. Programar en map-reduce un programa que genere como resultado un listado de tipo \n",
    "#(patron, frecuencia) indicando cuántas veces aparece cada patrón en la colección de documentos. Usar combiners \n",
    "#como método de agregación.  (**) (15 pts)\n",
    "\n",
    "docsRDD = sc.parallelize([\"My sister is the best singer ever\", \"A dragons ejects fire through its mouth\", \n",
    "                          \"My grandfather is ninety years old\", \"I have never purchased anything on amazon\"], 2)\n",
    "LETTERS = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"ñ\", \"o\", \"p\", \"q\",\n",
    "              \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "def pattern(word):\n",
    "    read = {}\n",
    "    word_pattern = \"\"\n",
    "    i = 0\n",
    "    for c in word:\n",
    "        if c not in read.keys():\n",
    "            word_pattern += LETTERS[i]\n",
    "            read[c] = i\n",
    "            i += 1\n",
    "        else:\n",
    "            word_pattern += LETTERS[read[c]]\n",
    "    return word_pattern\n",
    "\n",
    "#Obtengo la lista de pares (patron, frecuencia)\n",
    "patternList = docsRDD.flatMap(lambda row: [(pattern(word),1) for word in row.split()]).reduceByKey(lambda row1, row2: row1 + row2)\n",
    "print \"TODOS:\"\n",
    "print patternList.collect()\n",
    "\n",
    "#Obtengo el patron mas frecuente:\n",
    "mostFrequent = patternList.reduce(lambda row1, row2: row1 if row1[1] > row2[1] else row2)\n",
    "print \"MAS FRECUENTE:\"\n",
    "print mostFrequent\n",
    "\n",
    "#Obtengo el top five de los pares (patron, frecuencia):\n",
    "topFive = patternList.takeOrdered(5, lambda row: -row[1])\n",
    "print \"TOP FIVE:\" + str(topFive)\n",
    "\n",
    "orderedAlpha = patternList.sortByKey().collect()\n",
    "print \"ORDENADOS ALFABÉTICAMENE:\" + str(orderedAlpha)\n",
    "\n",
    "inverseRank = patternList.sortBy(lambda row: row[1]).take(5)\n",
    "print \"INVERSE RANK:\" + str(inverseRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 9.703703703703704), (4, 19.788732394366196), (3, 14.81578947368421)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hpData = [\"October arrived, spreading a damp chill over the grounds and into the castle.\", \n",
    "             \"Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students.\",\n",
    "             \"Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward\",\n",
    "             \"Ginny Weasley, who had been looking pale, was bullied into taking some by Percy\",\n",
    "             \"The steam pouring from under her vivid hair gave the impression that her whole head was on fire.\"\n",
    "            ]\n",
    "hpRDD = sc.parallelize(hpData, 2)\n",
    "#1st approach: supongo que me pide las frases posibles con estas longitudes dentro de los documentos:\n",
    "\n",
    "def caracteresPorFrase(phrase, lengths):\n",
    "    character_count_by_length = {}\n",
    "    words = phrase.split()\n",
    "    word_count = len(words)\n",
    "    for phrase_length in lengths:\n",
    "        i = 0\n",
    "        j = phrase_length-1\n",
    "        char_count = 0\n",
    "        #voy tomando palabras de a grupos de longitud phrase_length\n",
    "        while (j < word_count):\n",
    "            #estoy suponiendo frases lindas unidas por 1 único espacio\n",
    "            phrase = (\" \").join([ words[k] for k in xrange(i, j+1) ])\n",
    "            char_count += len(phrase)\n",
    "            \n",
    "            i += 1\n",
    "            j += 1\n",
    "        \n",
    "        #i es el indice inferior de cada frase que tomamos, asi que lo usamos como contador\n",
    "        #para calcular el promedio de las longitudes que sumamos en el loop\n",
    "        character_count_by_length[phrase_length] = (char_count, i+1)\n",
    "    \n",
    "    return [ (phrase_length, avg) for phrase_length, avg in character_count_by_length.items()]\n",
    "\n",
    "hpRDD.flatMap(lambda row: caracteresPorFrase(row, [i for i in xrange(2,5)]))\\\n",
    "     .reduceByKey(lambda row1, row2: (row1[0] + row2[0], row1[1] + row2[1]))\\\n",
    "     .map(lambda row: (row[0], row[1][0]/float(row[1][1]))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('www.twitter.com', (12000, datetime.date(2015, 10, 19))),\n",
       " ('www.facebook.com', (1700, datetime.date(2015, 10, 17))),\n",
       " ('www.bing.com', (170, datetime.date(2015, 10, 17))),\n",
       " ('www.google.com', (1300, datetime.date(2015, 10, 18)))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1) Se tiene un archivo con información sobre visitas a páginas web de la forma: (URL, visitas, fecha).\n",
    "#Existe solo un registro por día para cada URL. Se quiere generar un archivo que, por cada URL, \n",
    "#indique cuál fue la fecha en la que tuvo mas visitas y la cantidad de visitas. \n",
    "#Programar lo pedido en Map Reduce usando agregación para minimizar la cantidad de datos que deben \n",
    "#transferirse en la red. Atención: La resolución es muy simple, trivial, asi que es fundamental resolver \n",
    "#la agregación para el puntaje completo. (**) (15 pts)\n",
    "\n",
    "import datetime\n",
    "\n",
    "urls = [(\"www.google.com\", 1000, datetime.date(2015, 10, 17)),\n",
    "          (\"www.google.com\", 1300, datetime.date(2015, 10, 18)),\n",
    "          (\"www.google.com\", 1254, datetime.date(2015, 10, 19)),\n",
    "          (\"www.facebook.com\", 1700, datetime.date(2015, 10, 17)),\n",
    "          (\"www.facebook.com\", 1100, datetime.date(2015, 10, 18)),\n",
    "          (\"www.facebook.com\", 1204, datetime.date(2015, 10, 19)),\n",
    "          (\"www.twitter.com\", 1700, datetime.date(2015, 10, 17)),\n",
    "          (\"www.twitter.com\", 1100, datetime.date(2015, 10, 18)),\n",
    "          (\"www.twitter.com\", 12000, datetime.date(2015, 10, 19)),\n",
    "          (\"www.bing.com\", 170, datetime.date(2015, 10, 17)),\n",
    "          (\"www.bing.com\", 100, datetime.date(2015, 10, 18)),\n",
    "          (\"www.bing.com\", 12, datetime.date(2015, 10, 19))]\n",
    "\n",
    "urlsRDD = sc.parallelize(urls, 2)\n",
    "\n",
    "urlsRDD.map(lambda row: (row[0], (row[1], row[2])) ).reduceByKey(lambda row1, row2: row1 if (row1[0] > row2[0]) else row2).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('user1', 'www.facebook.com'): 1,\n",
       "             ('user1', 'www.google.com'): 2,\n",
       "             ('user1', 'www.twitter.com'): 1,\n",
       "             ('user2', 'www.despegar.com'): 1,\n",
       "             ('user2', 'www.facebook.com'): 1,\n",
       "             ('user2', 'www.google.com'): 3,\n",
       "             ('user3', 'www.almundo.com'): 1,\n",
       "             ('user3', 'www.booking.com'): 1,\n",
       "             ('user3', 'www.despegar.com'): 1,\n",
       "             ('user3', 'www.google.com'): 1,\n",
       "             ('user3', 'www.trivago.com'): 1,\n",
       "             ('user4', 'www.facebook.com'): 1,\n",
       "             ('user4', 'www.google.com'): 1,\n",
       "             ('user4', 'www.oxforddictionaries.com'): 2,\n",
       "             ('user4', 'www.rae.com'): 1,\n",
       "             ('user4', 'www.twitter.com'): 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se tiene un archivo de texto con datos de la forma (url_id, user_id) representando visitas a un website. \n",
    "#(Se pueden repetir registros). Se pide programar un script que indique cual es el \n",
    "#promedio global de sitios visitados por usuario. (***) (15pts)\n",
    "\n",
    "usersSites= [(\"user1\",\"www.google.com\"),\n",
    "            (\"user1\",\"www.google.com\"),\n",
    "            (\"user1\",\"www.facebook.com\"),\n",
    "            (\"user1\",\"www.twitter.com\"),\n",
    "            (\"user2\",\"www.google.com\"),\n",
    "            (\"user2\",\"www.facebook.com\"),\n",
    "            (\"user2\",\"www.google.com\"),\n",
    "            (\"user2\",\"www.google.com\"),\n",
    "            (\"user2\",\"www.despegar.com\"),\n",
    "            (\"user3\",\"www.booking.com\"),\n",
    "            (\"user3\",\"www.almundo.com\"),\n",
    "            (\"user3\",\"www.despegar.com\"),\n",
    "            (\"user3\",\"www.trivago.com\"),\n",
    "            (\"user3\",\"www.google.com\"),\n",
    "            (\"user4\",\"www.google.com\"),\n",
    "            (\"user4\",\"www.rae.com\"),\n",
    "            (\"user4\",\"www.oxforddictionaries.com\"),\n",
    "            (\"user4\",\"www.oxforddictionaries.com\"),\n",
    "            (\"user4\",\"www.facebook.com\"),\n",
    "            (\"user4\",\"www.twitter.com\")]\n",
    "\n",
    "usersSitesRDD = sc.parallelize(usersSites, 2)\n",
    "\n",
    "usersSitesRDD.distinct().map(lambda row: (row[0], 1))\\\n",
    "                        .reduceByKey(lambda row1, row2: row1 + row2)\\\n",
    "                        .flatMap(lambda row: [row[1]])\\\n",
    "                        .mean()\n",
    "            \n",
    "#Quiero contar cuantos registros distintos tengo:\n",
    "usersSitesRDD.distinct().count()\n",
    "\n",
    "usersSitesRDD.countByValue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ([0, 0, 0, 2, 4, 2, 0], 8))]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Un sitio de ventas online registra un log de los productos visitados por los \n",
    "#usuarios en un RDD de Spark con registros de la forma (user_id, product_id,date). Existe un registro por cada \n",
    "#visita de cada producto. Se quiere obtener un listado de los productos que se han visitado más de 500 veces \n",
    "#y para los cuales el día de la semana con mayor cantidad de visitas es el viernes.\n",
    "\n",
    "#para poder probar vamos a usar cantidad de visitas mínimas = 5\n",
    "\n",
    "visitedProducts = [(\"euge123\",1,'viernes'), (\"euge123\",2,'lunes'), (\"euge123\",3,'sabado'),\n",
    "                  (\"abrden\",4,'viernes'), (\"abrden\",2,'lunes'), (\"abrden\",3,'sabado'),\n",
    "                  (\"czarnia\",4,'viernes'), (\"czarnia\",2,'martes'), (\"czarnia\",1,'sabado'),\n",
    "                  (\"euge123\",4,'viernes'), (\"mporto\",3,'jueves'), (\"mporto\",1,'sabado'),\n",
    "                  (\"slazzari\",2,'viernes'), (\"slazzari\",1,'jueves'), (\"slazzari\",1,'viernes'),\n",
    "                  (\"abrden\",2,'domingo'), (\"euge123\",1,'jueves'), (\"slazzari\",1,'viernes'),\n",
    "                  (\"mporto\",3,'domingo'), (\"mporto\",4,'sabado'), (\"slazzari\",1,'viernes')]\n",
    "\n",
    "prodsRDD = sc.parallelize(visitedProducts, 2)\n",
    "\n",
    "def joinDictionaries(row1,row2):\n",
    "    for day, visit_count in row2.items():\n",
    "        if row1.has_key(day):\n",
    "            row1[day] += visit_count\n",
    "        else:\n",
    "            row1[day] = visit_count\n",
    "    return row1\n",
    "\n",
    "def hasMinimumValueSum(visits_per_day, n_visits):\n",
    "    count = 0\n",
    "    for visit_count in visits_per_day.values():\n",
    "        count += visit_count\n",
    "    return count > n_visits\n",
    "    \n",
    "def hasMaxCountOnKey(visits_per_day, day):\n",
    "    max_val = visits_per_day[day]\n",
    "    for week_day in visits_per_day.keys():\n",
    "        if (day != week_day) and (visits_per_day[week_day] > max_val):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "prodsRDD.map(lambda row: (row[1], {row[2] : 1} ))\\\n",
    "        .reduceByKey(lambda row1, row2: joinDictionaries(row1,row2))\\\n",
    "        .filter(lambda row: hasMinimumValueSum(row[1], 5) and hasMaxCountOnKey(row[1], 'viernes'))\\\n",
    "        .collect()\n",
    "\n",
    "#Otro enfoque:\n",
    "#.reduceByKey(lambda row1, row2: (row1[0][x] + row2[0][x]) for x in xrange(0,len(row1)) )\n",
    "#.filter(lambda row: (row[1][1] > 500) and ( row[1][0][VIERNES] >= x for x in row[1][0]))\\\n",
    "\n",
    "VIERNES = 4\n",
    "DATES = [\"lunes\", \"martes\", \"miercoles\", \"jueves\", \"viernes\", \"sabado\", \"domingo\"]\n",
    "prodsRDD = sc.parallelize(visitedProducts, 2)\n",
    "prodsRDD.map(lambda row: (row[1], ([ 1 if (row[2] == day) else 0  for day in DATES ] , 1)) )\\\n",
    "        .reduceByKey(lambda row1, row2: ( [row1[0][x] + row2[0][x] for x in xrange(0,len(row1[0]))], row1[1]+ row2[1] ) )\\\n",
    "        .filter(lambda row: (row[1][1] > 5) and ( row[1][0][VIERNES] >= x for x in row[1][0]))\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 193.0 failed 1 times, most recent failure: Lost task 0.0 in stage 193.0 (TID 324, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'dict'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'dict'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-4888dca41df7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlibrosRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mlibrosRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow2\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \"\"\"\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 193.0 failed 1 times, most recent failure: Lost task 0.0 in stage 193.0 (TID 324, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'dict'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 2346, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/pyspark/rdd.py\", line 1776, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/eugenia/spark-1.6.0-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'dict'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:342)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Se tiene un RDD con libros en donde cada registro es un texto.  \n",
    "#Se pide obtener todos los anagramas de mas de 7 letras que puedan encontrarse.  \n",
    "#El formato de salida debe ser una lista de listas en donde cada lista tiene un conjunto de palabras que son anagramas.\n",
    "#Ejemplo: [[discounter,introduces,reductions],[percussion,supersonic]...] (****) (15 pts)\n",
    "\n",
    "librosData = [(\"Decir que Inglaterra es el mejor pais es una presuposicion, Alemania lidera la economia mundial\" ),\n",
    "              (\"Las fresas le producen alergia, cuando las come delira y dice frases incoherentes\"),\n",
    "              (\"La superposicion de papeles dificultaba las cosas\"),\n",
    "              (\"Integrarla al mundo sera una tarea dificil\"),\n",
    "              (\"Los Enamoramientos se dan armoniosamente en Roma que es la ciudad del Amor\"),\n",
    "              (\"Energeticamente entro en la habitacion\"),\n",
    "              (\"genericamente\")]\n",
    "\n",
    "#.filter(lambda len_word_list: len_word_list[0] > 7)\\\n",
    "def is_anagram(word1, word2):\n",
    "    char_list = word1.lower().split()\n",
    "    for i in xrange(0, len(word2)):\n",
    "        if word2[i] in char_list:\n",
    "            char_list.pop(i)\n",
    "    return char_list == []\n",
    "\n",
    "librosRDD = sc.parallelize(librosData,2)\n",
    "librosRDD.flatMap(lambda texto: texto.split())\\\n",
    "         .map(lambda word: (len(word),[word]))\\\n",
    "         .reduceByKey(lambda row1, row2: row1 + row2)\\\n",
    "         .collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
